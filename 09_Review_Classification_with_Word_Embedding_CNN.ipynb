{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PoorneshShiva/Deep-learning-assigments/blob/main/09_Review_Classification_with_Word_Embedding_CNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Word2vec Word Embeddings\n",
        "\n",
        "**Word Embeddings** : They are a real-valued vector representation of words that allows words with the same meaning to have similar representation. Thus we can say word embeddings are the projection of meanings of words in a real-valued vector\n",
        "\n",
        "Word2vec is a Word Embedding Technique published in 2013. The word2vec algorithm uses a neural network model to learn word associations from a large corpus of text.\n",
        "\n",
        "It is the representation of words that allows words with the same meaning to have similar representation, Word2vec operationalizes this by projecting the meaning of the words in a vector space where words with similar meanings will tend to cluster together, and works with very different meanings are far from one another.\n",
        "\n",
        "**Using Pre-trained word2vec word embeddings** <br>\n",
        "Training your own word embeddings is a pretty expensive process (in terms of both time and computing). Thankfully, for many scenarios, it’s not necessary to train your own embeddings Someone has done the hard work of training word embeddings on a large corpus, such as Wikipedia, news articles, or even the entire web, and has put words and their corresponding vectors on the web. These embeddings\n",
        "can be downloaded and used to get the vectors for the words you want.  \n",
        "\n",
        "Some of the most popular pre-trained embeddings are Word2vec by Google, GloVe by Stanford, and fasttext embeddings by Facebook, to name a few.\n",
        "\n",
        "Below code, cell demonstrates how to use pre-trained word2vec word embeddings."
      ],
      "metadata": {
        "id": "QndRrO5Hxc1L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim\n",
        "import gensim.downloader as api\n",
        "\n",
        "# you can load a pre-trained Word Embedding model from genism-data\n",
        "# this will take awhile\n",
        "Word2VecModel = api.load(\"word2vec-google-news-300\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i_eqQTKNxh_W",
        "outputId": "cf1edecb-9bd4-44c5-816d-d307f1efa97c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.10/dist-packages (4.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.18.5 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.26.4)\n",
            "Requirement already satisfied: scipy<1.14.0,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from gensim) (1.13.1)\n",
            "Requirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.10/dist-packages (from gensim) (7.0.5)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open>=1.8.1->gensim) (1.16.0)\n",
            "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(Word2VecModel.most_similar('good'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1guSbVrJyBqU",
        "outputId": "62ea0e06-45ca-4d9b-a018-4be441de47b8"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('great', 0.7291510105133057), ('bad', 0.7190051078796387), ('terrific', 0.6889115571975708), ('decent', 0.6837348341941833), ('nice', 0.6836092472076416), ('excellent', 0.644292950630188), ('fantastic', 0.6407778263092041), ('better', 0.6120728850364685), ('solid', 0.5806034803390503), ('lousy', 0.576420247554779)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(Word2VecModel['good'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wvdw-J7S4Qi3",
        "outputId": "2fc58520-3c2c-41ba-943b-36ab2b3ebb2e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 0.04052734  0.0625     -0.01745605  0.07861328  0.03271484 -0.01263428\n",
            "  0.00964355  0.12353516 -0.02148438  0.15234375 -0.05834961 -0.10644531\n",
            "  0.02124023  0.13574219 -0.13183594  0.17675781  0.27148438  0.13769531\n",
            " -0.17382812 -0.14160156 -0.03076172  0.19628906 -0.03295898  0.125\n",
            "  0.25390625  0.12695312 -0.15234375  0.03198242  0.01135254 -0.01361084\n",
            " -0.12890625  0.01019287  0.23925781 -0.08447266  0.140625    0.13085938\n",
            " -0.04516602  0.06494141  0.02539062  0.05615234  0.24609375 -0.20507812\n",
            "  0.23632812 -0.00860596 -0.02294922  0.05078125  0.10644531 -0.03564453\n",
            "  0.08740234 -0.05712891  0.08496094  0.23535156 -0.10107422 -0.03564453\n",
            " -0.04736328  0.04736328 -0.14550781 -0.10986328  0.14746094 -0.23242188\n",
            " -0.07275391  0.19628906 -0.37890625 -0.07226562  0.04833984  0.11914062\n",
            "  0.06103516 -0.12109375 -0.27929688  0.05200195  0.04907227 -0.02709961\n",
            "  0.1328125   0.03369141 -0.32226562  0.04223633 -0.08789062  0.15429688\n",
            "  0.09472656  0.10351562 -0.02856445  0.00128174 -0.00427246  0.24609375\n",
            " -0.05957031 -0.16894531 -0.09619141  0.16796875  0.0133667   0.04882812\n",
            "  0.08349609  0.06347656 -0.00872803 -0.08642578 -0.03857422 -0.08251953\n",
            "  0.15722656  0.22753906 -0.00762939 -0.19921875 -0.06347656  0.12792969\n",
            " -0.06347656 -0.03027344  0.0456543   0.06298828 -0.02526855 -0.06787109\n",
            " -0.01141357 -0.13574219  0.02978516  0.10400391 -0.15917969 -0.08447266\n",
            "  0.29882812 -0.12597656  0.11425781 -0.08105469 -0.09082031 -0.07910156\n",
            " -0.11181641 -0.09619141  0.02770996  0.14257812 -0.26757812 -0.09375\n",
            "  0.03979492 -0.17871094 -0.02819824  0.01464844 -0.31640625 -0.24511719\n",
            " -0.08935547  0.09716797 -0.00964355 -0.14746094  0.15234375  0.21582031\n",
            "  0.05981445  0.23828125 -0.05151367  0.14941406  0.13574219 -0.03222656\n",
            " -0.265625   -0.11181641 -0.23046875 -0.140625    0.25585938 -0.15429688\n",
            "  0.1796875   0.15527344 -0.21582031  0.36328125 -0.1015625   0.04980469\n",
            "  0.07177734 -0.14550781 -0.03198242  0.00952148 -0.12109375  0.12109375\n",
            "  0.09765625  0.07763672  0.3203125  -0.22265625 -0.08447266 -0.10742188\n",
            "  0.11279297 -0.13867188 -0.21875     0.0145874   0.13378906 -0.00921631\n",
            "  0.00921631  0.16894531  0.16894531 -0.078125   -0.00665283  0.03735352\n",
            " -0.10888672 -0.25390625  0.01452637 -0.09716797 -0.19628906 -0.01782227\n",
            " -0.28125    -0.02050781 -0.02905273 -0.09375    -0.17675781  0.21484375\n",
            " -0.05224609 -0.11572266 -0.01977539 -0.10839844 -0.01342773 -0.15332031\n",
            " -0.140625   -0.11816406  0.09228516  0.109375    0.05761719 -0.03466797\n",
            "  0.03564453 -0.12011719 -0.14257812 -0.00072479 -0.06689453  0.11914062\n",
            " -0.10449219  0.07861328 -0.12792969  0.09570312 -0.00817871  0.07128906\n",
            "  0.20703125 -0.03149414  0.09570312  0.17285156 -0.07958984 -0.02429199\n",
            " -0.07519531 -0.07568359  0.09521484 -0.06494141 -0.00689697 -0.09033203\n",
            "  0.03100586  0.19921875 -0.10644531 -0.11474609  0.18652344 -0.05078125\n",
            "  0.0859375   0.00128937 -0.18847656 -0.20019531 -0.02832031  0.11328125\n",
            "  0.25976562  0.22070312  0.04101562  0.00171661  0.07568359 -0.01196289\n",
            "  0.0177002  -0.05883789 -0.25976562 -0.234375   -0.04956055  0.25976562\n",
            "  0.15332031  0.15136719  0.08300781 -0.15527344  0.04931641  0.07519531\n",
            " -0.05078125 -0.1328125  -0.13574219  0.04199219 -0.14257812  0.02099609\n",
            "  0.07861328  0.01611328  0.01623535 -0.21582031  0.01599121 -0.04882812\n",
            " -0.02404785  0.13476562  0.08496094 -0.01196289  0.10009766 -0.13867188\n",
            "  0.08056641 -0.22070312 -0.12011719  0.18945312  0.05444336 -0.05053711\n",
            "  0.00147247  0.14160156 -0.06494141 -0.05566406 -0.09033203 -0.0267334\n",
            " -0.10498047  0.02416992  0.01422119  0.1875     -0.16503906  0.01538086\n",
            " -0.04174805  0.05444336 -0.01184082 -0.15625     0.00193024 -0.06982422]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training our own embeddings\n",
        "\n",
        "Now we’ll focus on training our own word embeddings. For this, we’ll look at two architectural variants that were proposed in the original Word2vec approach. The two variants are:\n",
        "\n",
        "1. Continuous bag of words (CBOW)\n",
        "2. SkipGram\n",
        "\n",
        "Both of these have a lot of similarities in many respects.\n",
        "\n",
        "Throughout this section, we’ll use the sentence “The quick brown fox jumps over the lazy dog” as our example text.\n",
        "\n",
        "**1. Continuous bag of words (CBOW)**\n",
        "\n",
        "In CBOW, the primary task is to build a language model that correctly predicts the center word given the context words in which the center word appears. Consider our example sentence we take the word “jumps” as the center word, then its context is formed by words in its vicinity. If we take the context size of 2, then for our example, the context is given by brown, fox, over, the. CBOW uses the context words to predict the target word—jumps—as shown in the below figure\n",
        "<br><br>\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1-RJkuhoYbKnp5nt481Omxg0GlZJW6MgD\">\n",
        "\n",
        "<br><br>\n",
        "Now next task is to create a training sample of the form (X, Y) for this task where X will be context words and Y will be Center word. We define the value of context window = 2 in this case.\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1-SHpn2rS1-Qd8Nz-mtczRlb-84e8nwNd\">\n",
        "\n",
        "<br><br>\n",
        "Now that we have the training data ready, let’s focus on the model. For this, we construct a shallow net (it’s shallow since it has a single hidden layer). We assume we want to learn D-dim word embeddings. Further, let V be the vocabulary of the text corpus\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1-T-XNzSrMKzXEkF3fzNx1VaKnroZ76yP\">\n",
        "\n",
        "<br><br>\n",
        "The objective is to learn an embedding matrix E|V| x d.To begin with, we initialize the matrix randomly. Here, |V| is the size of corpus vocabulary and d is the dimension of the embedding. Let’s break down the shallow net in Figure layer by layer. In the input layer, indices of the words in context are used to fetch the corresponding rows from the embedding matrix E|V| x d. The vectors fetched are then added to get a single D-dim vector, and this is passed to the next layer. The next layer simply takes this d vector and multiplies it with another matrix E’d x |V|.. This gives a 1 x |V| vector, which is fed to a softmax function to get probability distribution over the vocabulary space. This distribution is compared with the label and uses backpropagation to update both the matrices E and E’ accordingly. At the end of the training, E is the embedding matrix we wanted to learn."
      ],
      "metadata": {
        "id": "7U4_LEK87TW6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2. SkipGram**\n",
        "\n",
        "SkipGram is very similar to CBOW, with some minor changes. In Skip‐ Gram, the task is to predict the context words from the center word. For our toy corpus with context size 2, using the center word “jumps,” we try to predict every word in context—“brown,” “fox,” “over,” “the”—as shown in the Figure below\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1-hWnCt1IzcArk_dzCTBOt8tqMehDwI9t\">\n",
        "\n",
        "Now we will create a training sample of the form (X, Y) for this task where X will be the center word and Y will be Context words.\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1-W9PbbUt7LnnBSd9EpDfKRXVmp6d8nFR\">\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?id=1-hlD6fev7YSxm_bauLoDLFeAR1ul7YXA\">\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "\n",
        "The shallow network used to train the SkipGram model, shown in the below Figure, is very similar to the network used for CBOW, with some minor changes. In the input layer, the index of the word in the target is used to fetch the corresponding row from the embedding matrix E|V| x d. The vectors fetched are then passed to the next layer. The next layer simply takes this d vector and multiplies it with another matrix E’d x |V|. This gives a 1 x |V| vector, which is fed to a softmax function to get probability distribution over the vocabulary space. This distribution is compared with the label and uses backpropagation to update both the matrices E and E’ accordingly. At the end of the training, E is the embedding matrix we wanted to learn."
      ],
      "metadata": {
        "id": "IC0xFpmb7bAE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Implementation** :\n",
        "\n",
        "One of the most commonly used implementations is with gensim. We have to choose several hyperparameters (i.e., the variables that need to be set before starting the training process). Let’s look at two examples.\n",
        "\n",
        "Dimensionality of the word vectors\n",
        "\n",
        "As the name indicates, this decides the space of the learned embeddings. While there is no ideal number, it’s common to construct word vectors with dimensions in the range of 50–500 and evaluate them on the task we’re using them for to choose the best option. In gensim we do this by setting the \"size\" parameter to the size we want.\n",
        "\n",
        "Context window\n",
        "\n",
        "How long or short the context we look for to learn the vector representation is. In gensim we do this by setting the \"window\" parameter to the size we want.\n",
        "\n",
        "There are also other choices we make, such as whether to use CBOW or SkipGram to\n",
        "learn the embeddings.\n",
        "\n",
        "**Next few code cells shows implementation of CBOW**"
      ],
      "metadata": {
        "id": "m_8dQwJV7iBS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.test.utils import common_texts\n",
        "from gensim.models.word2vec import Word2Vec\n",
        "\n",
        "print(\"Sentences on which We are gonna train our CBOW Word2Vec Model:\\n\")\n",
        "print(common_texts)\n",
        "\n",
        "Our_CBOW_Word2Vec_Model = Word2Vec(common_texts, size = 10, window = 5, min_count = 1, workers = 8, sg = 0)\n",
        "Our_CBOW_Word2Vec_Model.save(\"Our_CBOW_Word2Vec_Model.w2v\")\n",
        "print(\"Model Saved\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "id": "KwXVzKFI7TGm",
        "outputId": "94a3099f-b1a8-4d02-8d96-011e493fb4c3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentences on which We are gonna train our CBOW Word2Vec Model:\n",
            "\n",
            "[['human', 'interface', 'computer'], ['survey', 'user', 'computer', 'system', 'response', 'time'], ['eps', 'user', 'interface', 'system'], ['system', 'human', 'system', 'eps'], ['user', 'response', 'time'], ['trees'], ['graph', 'trees'], ['graph', 'minors', 'trees'], ['graph', 'minors', 'survey']]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "Word2Vec.__init__() got an unexpected keyword argument 'size'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-428a0292d3d3>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommon_texts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mOur_CBOW_Word2Vec_Model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommon_texts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mworkers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mOur_CBOW_Word2Vec_Model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Our_CBOW_Word2Vec_Model.w2v\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Model Saved\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Word2Vec.__init__() got an unexpected keyword argument 'size'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Our_CBOW_Word2Vec_Model.wv.most_similar('human', topn = 5)"
      ],
      "metadata": {
        "id": "A9gh55ae7xlu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Our_CBOW_Word2Vec_Model.wv['human']"
      ],
      "metadata": {
        "id": "hTIgx_oC7xI2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Glove Word Embeddings\n",
        "\n",
        "GloVe Stands for Global Vectors for word representation is another word embedding technique that was developed as an open-source project at Stanford and was launched in 2014. Just to refresh, word vectors put words to a nice vector space, where similar words cluster together and different words repel. The advantage of GloVe is that, unlike Word2vec, GloVe does not rely just on local statistics (local context information of words), but incorporates global statistics (word co-occurrence) to obtain word vectors. But keep in mind that there’s quite a bit of synergy between the GloVe and Word2vec. The gloVe can be used to find relations between words like synonyms, company-product relations, zip codes, and cities, etc.\n",
        "\n",
        "The question may arise Why do we need Glove if we have word2vec as a good word embedding technique Because Word2vec relies only on local information of language. That is, the semantics learned for a given word, are only affected by the surrounding words.\n",
        "\n",
        "For example, take the sentence,\n",
        "\n",
        "The cat sat on the mat\n",
        "\n",
        "If you use Word2vec, it wouldn’t capture information like,\n",
        "\n",
        "is “the” a special context of the words “cat” and “mat” ?\n",
        "\n",
        "or\n",
        "\n",
        "is “the” just a stopword?\n",
        "\n",
        "This can be suboptimal, especially in the eye of theoreticians.\n",
        "\n",
        "GloVe method is built on an important idea, You can derive semantic relationships between words from the co-occurrence matrix. Given a corpus having V words, the co-occurrence matrix X will be a V x V matrix, where the i th row and j th column of X, X_ij denotes how many times word i has co-occurred with word j. An example co-occurrence matrix might look as follows.\n",
        "\n",
        "![](https://miro.medium.com/max/434/1*QWcK8CIDs8kMkOwsOxvywA.png)\n",
        "\n",
        "The co-occurrence matrix for the sentence “the cat sat on the mat” with a window size of 1. As you probably noticed it is a symmetric matrix.\n",
        "\n",
        "For detailed knowledge about Glove word embedding, you can refer [This article](https://towardsdatascience.com/light-on-math-ml-intuitive-guide-to-understanding-glove-embeddings-b13b4f19c010)"
      ],
      "metadata": {
        "id": "1n5i7NzR72en"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/allenai/spv2/raw/master/model/glove.6B.100d.txt.gz\n",
        "!gzip -d glove.6B.100d.txt.gz"
      ],
      "metadata": {
        "id": "xNnJIXsGDr_2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!tail glove.6B.100d.txt"
      ],
      "metadata": {
        "id": "x25yFqyeEGJ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.scripts.glove2word2vec import glove2word2vec\n",
        "\n",
        "# convert GloVe file to Word2Vec file\n",
        "word2vec_output_file = 'glove.6B.100d.txt.word2vec'\n",
        "glove2word2vec(\"glove.6B.100d.txt\", word2vec_output_file)"
      ],
      "metadata": {
        "id": "_p-DW-5h79Eo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import KeyedVectors\n",
        "\n",
        "# load the Stanford GloVe model\n",
        "glove_model = KeyedVectors.load_word2vec_format(word2vec_output_file, binary=False)"
      ],
      "metadata": {
        "id": "sCH6_aJw79pI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Most similar words to word 'human' : \")\n",
        "glove_model.most_similar('human')"
      ],
      "metadata": {
        "id": "2KReyowc7_MY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Glove Word Embeddings of word 'human' \")\n",
        "glove_model['human']"
      ],
      "metadata": {
        "id": "v26UL_lY8Aw0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FastText Word Embeddings\n",
        "\n",
        "A word can be represented by its constituent character ngrams. Following a similar architecture to Word2vec, fastText learns embeddings for words and character n-grams together and views a word’s embedding vector as an aggregation of its constituent character n-grams. This makes it possible to generate embeddings even for words that are not present in the vocabulary. Say there’s a word, “gregarious,” that’s not found in the embedding’s word vocabulary. We break it into character n-grams—gre, reg, ega, ….ous—and combine these embeddings of the ngrams to arrive at the embedding of “gregarious.”\n",
        "\n",
        "How FastText Works?\n",
        "\n",
        "FastText is a modified version of word2vec (i.e.. Skip-Gram and CBOW). The only difference between fastText vs word2vec is its pooling strategies (what are the input, output, and dictionary of the model). In word2vec each word is represented as a bag of words but in FastText each word is represented as a bag of character n-gram.\n",
        "\n",
        "**character n-grams** the contiguous sequence of n items from a given sample of a character or word. It may be bigram, trigram, etc.\n",
        "For example character trigram (n = 3) of the word “where” will be:\n",
        "\n",
        "<wh, whe, her, ere, re>\n",
        "\n",
        "In FastText architecture, they have also included the word itself with the character n-gram. That means input data to the model for the word “eating” will be:\n",
        "\n",
        "![](https://amitness.com/images/fasttext-center-word-embedding.png)\n",
        "\n",
        "Now the model I am referring same is word2vec which is a shallow neural network with one hidden layer as discussed above.\n",
        "\n",
        "\n",
        "Now to prepare training data for the (Skip-Gram-based) FastText model, we define “context word” as the word which follows a given word in the text (which will be our “target word”). That means we will be predicting the surrounding word for a given word.\n",
        "\n",
        "Note: FastText word embeddings support both Continuous Bag of Words (CBOW) and Skip-Gram models. I will explain and implement the skip-gram model in the below cell to learn vector representation (FastText word embeddings). Now let’s construct our training examples (like Skip-Gram), scanning through the text with a window will prepare a context word and a target word.\n",
        "\n",
        "Consider the sentence :\n",
        "\n",
        "<div style = \"text-align:center\"><b> i like natural language processing</b></div>\n",
        "\n",
        "![](https://secureservercdn.net/45.40.148.234/um0.ec8.myftpupload.com/wp-content/uploads/2020/10/Picture2.png)\n",
        "\n",
        "For the above example, for context words “i” and “natural” the target word will be “like”. Full training data for FastText word embedding will look like below. By observing the below training data, your confusion of fastText vs word2vec should be clear.\n",
        "\n",
        "\n",
        "Now you know in word2vec (skip-gram) each word is represented as a bag of words but in FastText each word is represented as a bag of character n-gram. This training data preparation is the only difference between FastText word embeddings and skip-gram (or CBOW) word embeddings.\n",
        "\n",
        "After training data preparation of FastText, training the word embedding, finding word similarity, etc. are the same as the word2vec model (for our example similar to the skip-gram model).\n",
        "\n",
        "Now let’s see how to implement FastText word embeddings in python using Gensim library.\n"
      ],
      "metadata": {
        "id": "SMwr-1hS8DO8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://dl.fbaipublicfiles.com/fasttext/vectors-english/wiki-news-300d-1M.vec.zip\n",
        "!unzip wiki-news-300d-1M.vec.zip"
      ],
      "metadata": {
        "id": "mt9GrIxrF9tj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import FastText, KeyedVectors\n",
        "\n",
        "# this will take forever\n",
        "fasttext_model = KeyedVectors.load_word2vec_format('wiki-news-300d-1M.vec') # equals to api.load(\"fasttext-wiki-news-subwords-300\")\n",
        "print(\"Most similar words to word 'human': \")\n",
        "fasttext_model.most_similar('human')"
      ],
      "metadata": {
        "id": "RxRAHk0C8GZ0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"FastText Word Embeddings of the word 'human'\")\n",
        "fasttext_model['human']"
      ],
      "metadata": {
        "id": "CKhC34sK8JDQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training our own fasttext model using python's gensim library by settings up below listed hyperparameters:\n",
        "\n",
        "\n",
        "- size: Dimensionality of the word vectors.\n",
        "- window: window size.\n",
        "- min_count: The model ignores all words with total frequency lower than this.\n",
        "- sample: The threshold for configuring which higher-frequency words are randomly down sampled, useful range is (0, 1e-5).\n",
        "- workers: Use these many worker threads to train the model (=faster training with multicore machines).\n",
        "- sg: Training algorithm: skip-gram if sg=1, otherwise CBOW.\n",
        "- iter: Number of iterations (epochs) over the corpus."
      ],
      "metadata": {
        "id": "4zliUhUH8M_Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import FastText\n",
        "from gensim.test.utils import common_texts\n",
        "\n",
        "our_fasttext_model = FastText(common_texts, size = 100, min_count = 1, window = 5, sg = 1)"
      ],
      "metadata": {
        "id": "Ft2gN-_F8NyQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Most Similar words of word 'computer' : \")\n",
        "our_fasttext_model.wv.most_similar('computer')"
      ],
      "metadata": {
        "id": "vHrCSX088PK-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Word Embedding for the word \"computer\"\n",
        "our_fasttext_model.wv['computer']"
      ],
      "metadata": {
        "id": "qsagYcIm8R1Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Important Points Regarding Word Embeddings**\n",
        "\n",
        "1. All text representations are **inherently biased** based on what they saw in training data. For example, an embedding model trained heavily on technology news or articles is likely to identify Apple as being closer to, say, Microsoft or Facebook than to an orange or pear.\n",
        "\n",
        "2. Unlike the basic vectorization approaches, pre-trained embeddings are generally **large-sized files (several gigabytes)**, which may pose problems in certain deployment scenarios. This is something we need to address while using them, otherwise, it can become an engineering bottleneck in performance. The Word2vec model takes ~4.5 GB RAM."
      ],
      "metadata": {
        "id": "zmuCGr5v8hgq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Visualizing Embeddings\n",
        "\n",
        "\n",
        "So far, we’ve seen various vectorization techniques for representing text. The vectors obtained are used as features for the NLP task at hand. An important aspect of any ML project is feature exploration. Visual exploration is a very important aspect of any data-related problem. Even though embeddings are low-dimensional vectors, even\n",
        "100 or 300 dimensions are too high to visualize.\n",
        "\n",
        "t-SNE, or t-distributed Stochastic Neighboring Embedding help us solve this problem. It’s a technique used for visualizing high-dimensional data like embeddings by reducing them to two or three-dimensional data. The technique takes in the embeddings (or any data) and looks at how to best represent the input data using lesser dimensions, all while maintaining the same data distributions in original high-dimensional input space and low-dimensional output space. This, therefore, enables us to plot and visualize the input data. It helps to get a feel for the space of word embedding.\n",
        "\n",
        "Now lets plot scatter plots of most-similar words in the vocabulary of different embedding schemes."
      ],
      "metadata": {
        "id": "XfpdxwEi8ksa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing Word2vec Word Embedding\n",
        "keys = ['Germany', 'good', 'friday', 'science', 'Twitter', 'masters', 'computer', 'election', 'costly',\n",
        "        'learning', 'finance', 'machine', 'android', 'peace', 'nature', 'war']\n",
        "\n",
        "words_clusters = []\n",
        "embeddings_clusters = []\n",
        "\n",
        "for word in keys:\n",
        "    words = []\n",
        "    embeddings = []\n",
        "\n",
        "    for similar_word, _ in Word2VecModel.most_similar(word, topn = 30):\n",
        "        words.append(similar_word)\n",
        "        embeddings.append(Word2VecModel[word])\n",
        "    words_clusters.append(words)\n",
        "    embeddings_clusters.append(embeddings)\n"
      ],
      "metadata": {
        "id": "PORXnOgw8hKg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.manifold import TSNE\n",
        "import numpy as np\n",
        "\n",
        "embedding_array = np.array(embeddings_clusters)\n",
        "n, m, k = embedding_array.shape\n",
        "\n",
        "tsne_2d_model = TSNE(perplexity = 15, n_components = 2, n_iter = 4000, random_state = 11, init = 'pca')\n",
        "tsne_embeddings = np.array(tsne_2d_model.fit_transform(embedding_array.reshape(n * m, k))).reshape(n, m, 2)"
      ],
      "metadata": {
        "id": "j2MUyIPH8nGD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "%matplotlib inline\n",
        "\n",
        "def plot_most_similar_words(labels, embedding_cluster, word_cluster, title):\n",
        "\n",
        "    colors = cm.rainbow(np.linspace(0, 1, len(labels)))\n",
        "    plt.figure(figsize = (16,9))\n",
        "    for label, embeddings, words, color in zip(labels, embedding_cluster, word_cluster, colors):\n",
        "        x = embeddings[:, 0]\n",
        "        y = embeddings[:, 1]\n",
        "        plt.scatter(x, y, c=color, alpha=0.7, label=label)\n",
        "    plt.legend(loc = 4)\n",
        "    plt.title(title)\n",
        "    plt.grid(True)\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "VgmwyX8G8ojm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_most_similar_words(keys, tsne_embeddings, words_clusters, \"Visualizing Word2vec Word Embedding\")"
      ],
      "metadata": {
        "id": "aHUxOwL88qqG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f64bBXyhpxTR"
      },
      "source": [
        "# Text Classification - Deep Learning CNN Models\n",
        "\n",
        "<img src=\"https://github.com/dipanjanS/nlp_workshop_dhs18/blob/master/Unit%2012%20-%20Project%209%20-%20Sentiment%20Analysis%20-%20Supervised%20Learning/banner.jpeg?raw=1\">\n",
        "\n",
        "When it comes to text data, sentiment analysis is one of the most widely performed analysis on it. Sentiment Analysis has been through tremendous improvements from the days of classic methods to recent times where in the state of the art models utilize deep learning to improve the performance.\n",
        "\n",
        "Convolutional Neural Networks or CNNs are the work-horse of the deep learning world. They have, in some sense, brought deep learning research into mainstream discussions. The advancements in the image classification world has left even humans behind.\n",
        "\n",
        "<img src=\"https://github.com/dipanjanS/nlp_workshop_dhs18/blob/master/Unit%2012%20-%20Project%209%20-%20Sentiment%20Analysis%20-%20Supervised%20Learning/cnn.png?raw=1\">\n",
        "In this project, we will attempt at performing sentiment analysis utilizing the power of CNNs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iayiP3GZrjjk"
      },
      "source": [
        "!pip install contractions\n",
        "!pip install textsearch\n",
        "!pip install tqdm\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gbnh0egkUzRX"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# fix random seed for reproducibility\n",
        "seed = 42\n",
        "np.random.seed(seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_JEXODppxTc"
      },
      "source": [
        "## Load Movie Review Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U7pQ5WR1VYqm"
      },
      "source": [
        "dataset = pd.read_csv(r'https://github.com/dipanjanS/nlp_workshop_dhs18/raw/master/Unit%2011%20-%20Sentiment%20Analysis%20-%20Unsupervised%20Learning/movie_reviews.csv.bz2', compression='bz2')\n",
        "dataset.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XMP7r5cUV6-B"
      },
      "source": [
        "# take a peek at the data\n",
        "dataset.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset[\"sentiment\"].value_counts().plot(kind=\"bar\")"
      ],
      "metadata": {
        "id": "n9LgmxBEeUcK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CETaiGM2tO_d"
      },
      "source": [
        "# Text Wrangling & Normalization\n",
        "\n",
        "**Text Preprocessing** is traditionally an important step for **Natural Language Processing (NLP)** tasks. It transforms text into a more digestible form so that machine learning algorithms can perform better."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "stopwords = stopwords.words('english')\n",
        "stopwords"
      ],
      "metadata": {
        "id": "A6hNeqgLpcgL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tkFyu9u3tUOi"
      },
      "source": [
        "import contractions\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import tqdm\n",
        "import unicodedata\n",
        "\n",
        "from nltk.stem import SnowballStemmer\n",
        "stemmer = SnowballStemmer('english')\n",
        "\n",
        "def strip_html_tags(text):\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\n",
        "    [s.extract() for s in soup(['iframe', 'script'])]\n",
        "    stripped_text = soup.get_text()\n",
        "    stripped_text = re.sub(r'[\\r|\\n|\\r\\n]+', '\\n', stripped_text)\n",
        "    return stripped_text\n",
        "\n",
        "def remove_stopwords_and_stemming(text, stem):\n",
        "    tokens = []\n",
        "    for token in text.split():\n",
        "      if token not in stopwords:\n",
        "        # chops off the ends of words\n",
        "        if stem:\n",
        "          tokens.append(stemmer.stem(token))\n",
        "        else:\n",
        "          tokens.append(token)\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "def pre_process_corpus(docs, stem = False):\n",
        "    norm_docs = []\n",
        "    # tqdm to display a progess bar while looping\n",
        "    for doc in tqdm.tqdm(docs):\n",
        "        # remove HTML tags\n",
        "        doc = strip_html_tags(doc)\n",
        "\n",
        "        # convert tab, new lines to empty spaces\n",
        "        doc = doc.translate(doc.maketrans(\"\\n\\t\\r\", \"   \"))\n",
        "\n",
        "        # lowercase\n",
        "        doc = doc.lower()\n",
        "\n",
        "        # remove accented chars\n",
        "        doc = unicodedata.normalize('NFKD', doc).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "\n",
        "        # expand shortened words, e.g. don't to do not\n",
        "        doc = contractions.fix(doc)\n",
        "\n",
        "        # Replace all non alphabets.\n",
        "        doc = re.sub('[^a-zA-Z]', ' ', doc)\n",
        "\n",
        "        # Single character removal\n",
        "        doc = re.sub(r\"\\s+[a-zA-Z]\\s+\", ' ', doc)\n",
        "\n",
        "        # remove white spaces\n",
        "        doc = re.sub(' +', ' ', doc)\n",
        "        doc = doc.strip()\n",
        "\n",
        "        # remove stop words and apply stemming\n",
        "        doc = remove_stopwords_and_stemming(doc, stem)\n",
        "\n",
        "        norm_docs.append(doc)\n",
        "    return norm_docs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EgAqgSnHtap5"
      },
      "source": [
        "dataset['review'] = pre_process_corpus(dataset['review'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.head()"
      ],
      "metadata": {
        "id": "ood7aYFpnw2z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Positive Words"
      ],
      "metadata": {
        "id": "9sSLSXjkvnXy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from wordcloud import WordCloud\n",
        "\n",
        "plt.figure(figsize = (20,20))\n",
        "wc = WordCloud(max_words = 2000 , width = 1600 , height = 800).generate(\" \".join(dataset[dataset.sentiment == 'positive'].review))\n",
        "plt.imshow(wc , interpolation = 'bilinear')"
      ],
      "metadata": {
        "id": "_bmnvDwYioWx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Negative Words"
      ],
      "metadata": {
        "id": "uhcmaREQpKGM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize = (20,20))\n",
        "wc = WordCloud(max_words = 2000 , width = 1600 , height = 800).generate(\" \".join(dataset[dataset.sentiment == 'negative'].review))\n",
        "plt.imshow(wc , interpolation = 'bilinear')"
      ],
      "metadata": {
        "id": "jwFLHxPZo-r-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-qBM4Od_pxTp"
      },
      "source": [
        "## Prepare Train Test Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XpwueLn6V-qF"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Splits Dataset into Training and Testing set\n",
        "train_data, test_data = train_test_split(dataset, test_size=0.2)\n",
        "\n",
        "print(\"Train Data size:\", len(train_data))\n",
        "print(\"Test Data size\", len(test_data))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization\n",
        "To prepare text data for our deep learning model, we transform each review into a sequence.\n",
        "Every word in the review is mapped to an integer index and thus the sentence turns into a sequence of numbers. The process is called **Tokenization.**\n",
        "\n",
        "Given a character sequence and a defined document unit, tokenization is the task of chopping it up into pieces, called *tokens* , perhaps at the same time throwing away certain characters, such as punctuation.\n",
        "![Tokenization](https://cdn.analyticsvidhya.com/wp-content/uploads/2019/11/tokenization.png)\n",
        "\n",
        "`tokenizer` create tokens for every word in the data corpus and map them to a index using dictionary.\n",
        "\n",
        "`word_index` contains the index for each word\n",
        "\n",
        "`vocab_size` represents the total number of word in the data corpus"
      ],
      "metadata": {
        "id": "jcBcrpQnJfFI"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dff8sG63cw03"
      },
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "tokenizer = Tokenizer()\n",
        "# fit the tokenizer on the documents\n",
        "tokenizer.fit_on_texts(train_data.review)\n",
        "\n",
        "print(\"Vocabulary size: {}\".format(len(tokenizer.word_index)))\n",
        "print(\"Number of Documents: {}\".format(tokenizer.document_count))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0xd_eGZ1vQRR"
      },
      "source": [
        "# First and Last word\n",
        "max([(k, v) for k, v in tokenizer.word_index.items()], key = lambda x:x[1]), min([(k, v) for k, v in tokenizer.word_index.items()], key = lambda x:x[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4yv_m8T5c2xg"
      },
      "source": [
        "train_sequences = tokenizer.texts_to_sequences(train_data.review)\n",
        "test_sequences = tokenizer.texts_to_sequences(test_data.review)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQjiXA7Ntw13"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "train_lens = [len(s) for s in train_sequences]\n",
        "test_lens = [len(s) for s in test_sequences]\n",
        "\n",
        "fig, ax = plt.subplots(1,2, figsize=(12, 6))\n",
        "h1 = ax[0].hist(train_lens)\n",
        "h2 = ax[1].hist(test_lens)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfZwP6C8pxT8"
      },
      "source": [
        "### Sequence Normalization\n",
        "\n",
        "Not all reviews are of same length. To handle this difference in length of reviews, we define a maximum length.\n",
        "For reviews which are smaller than this length, we pad them with zeros which longer ones are truncated"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BtuGJ0wXjQnC"
      },
      "source": [
        "MAX_SEQUENCE_LENGTH = 1000"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wAnv99kzWA5k"
      },
      "source": [
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# pad dataset to a maximum review length in words\n",
        "X_train = pad_sequences(train_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "X_test = pad_sequences(test_sequences, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "X_train.shape, X_test.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9X_4ticSpxUC"
      },
      "source": [
        "### Encoding Labels\n",
        "\n",
        "The dataset contains labels of the form positive/negative. The following step encodes the labels using ```sklearn's``` ```LabelEncoder```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rRMaWb1ldqyl"
      },
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "le = LabelEncoder()\n",
        "num_classes = 2 # positive -> 1, negative -> 0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJjbtyDjfsd1"
      },
      "source": [
        "y_train = le.fit_transform(train_data.sentiment)\n",
        "y_test = le.transform(test_data.sentiment)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YCjRYBh2pxUM"
      },
      "source": [
        "## Prepare the Model\n",
        "\n",
        "Since textual data is a sequence of words, we utilize ```1D``` convolutions to scan through the sentences.\n",
        "The model first transforms each word into lower dimensional embedding/vector space followed by 1d convolutions and then passing the data through dense layers before the final layer for classification"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LR3mdd8kjgW1"
      },
      "source": [
        "EMBED_SIZE = 300\n",
        "EPOCHS = 2\n",
        "BATCH_SIZE = 128\n",
        "VOCAB_SIZE = len(tokenizer.word_index)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Flatten, Conv1D, MaxPooling1D, Embedding\n",
        "\n",
        "# create the model\n",
        "model = Sequential()\n",
        "model.add(Embedding(VOCAB_SIZE, EMBED_SIZE, input_length=MAX_SEQUENCE_LENGTH))\n",
        "\n",
        "# TODO: Continue building the CNN model\n",
        "\n",
        "# Add 3 Conv1D layers with 128, 64, 32 filters, a kernel size of 4, and same padding\n",
        "# Follow each Conv1D layer by a MaxPooling1D with a pool size of 2\n",
        "# Flat all inputs\n",
        "# A hidden layer with 256 units\n",
        "# The output\n",
        "model.add(Conv1D(filters=128, kernel_size=4, padding='same', activation='relu'))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "\n",
        "model.add(Conv1D(filters=64, kernel_size=4, padding='same', activation='relu'))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "\n",
        "\n",
        "model.add(Conv1D(filters=32, kernel_size=4, padding='same', activation='relu'))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "kDyaSt5SPdAi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "bb4f8454-6b67-48df-8a2c-26ee135a8fd2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'VOCAB_SIZE' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-0aa59d21db49>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# create the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mVOCAB_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEMBED_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMAX_SEQUENCE_LENGTH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# TODO: Continue building the CNN model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'VOCAB_SIZE' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "szNl8QiQpxUa"
      },
      "source": [
        "## Model Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "id": "0uc0jXszf5ob",
        "outputId": "af09c697-3f09-41d9-c3a9-391b2c822973"
      },
      "source": [
        "# TODO: Train the model with 2 epochs\n",
        "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=EPOCHS, batch_size=BATCH_SIZE)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-fdffb758bea8>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# TODO: Train the model with 2 epochs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cuKczZqYpxUk"
      },
      "source": [
        "## Model Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Zik9CWQgNlK",
        "outputId": "77cf8611-5777-42f2-c099-8e415e47bc01"
      },
      "source": [
        "# TODO: Evaluate the model on the test set and find accuracy\n",
        "model.evaluate(X_test, y_test)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 3s 11ms/step - loss: 0.3277 - accuracy: 0.8806\n",
            "Accuracy: 88.06%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B904TLKNiA1B",
        "outputId": "a32e8efa-7ec6-4366-c99a-6c736207e42d"
      },
      "source": [
        "predictions = (model.predict(X_test) > 0.5).astype(\"int32\")\n",
        "predictions[:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 2s 6ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0],\n",
              "       [1],\n",
              "       [0],\n",
              "       [1],\n",
              "       [0],\n",
              "       [1],\n",
              "       [1],\n",
              "       [0],\n",
              "       [0],\n",
              "       [0]], dtype=int32)"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_test[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H8yN1M3Ndoy8",
        "outputId": "b9be88fb-424e-4c5d-8aae-75869c002935"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 1, 0, 1, 0, 1, 1, 1, 0, 0])"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HVjbqjaopxU-",
        "outputId": "02eff52e-2d46-4029-ff52-0fbc470f500c"
      },
      "source": [
        "predictions = ['positive' if item == 1 else 'negative' for item in predictions]\n",
        "predictions[:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['negative',\n",
              " 'positive',\n",
              " 'negative',\n",
              " 'positive',\n",
              " 'negative',\n",
              " 'positive',\n",
              " 'positive',\n",
              " 'negative',\n",
              " 'negative',\n",
              " 'negative']"
            ]
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Print a classification report\n",
        "print(classification_report(test_data.sentiment, predictions))"
      ],
      "metadata": {
        "id": "y8t6CswYPtVI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train our own Embedding using Word2Vec"
      ],
      "metadata": {
        "id": "C1oaOeaVq39t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models.word2vec import Word2Vec\n",
        "\n",
        "sentences = [row.split() for row in train_data.review]\n",
        "word2vec_model = Word2Vec(sentences, size=EMBED_SIZE, window=5, min_count=1, workers=16)\n",
        "print(word2vec_model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m0myVBoMFTCw",
        "outputId": "c6df0f6a-cab2-44a7-a07f-6c03e04efcdb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word2Vec(vocab=92690, size=300, alpha=0.025)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(word2vec_model.wv.most_similar('mafia'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GKuTA_d5rf8w",
        "outputId": "c15ddb5c-c63c-478c-92dd-dbe45ec6977d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('mob', 0.8272112607955933), ('terrorising', 0.8021698594093323), ('murderers', 0.7973777055740356), ('kidnapping', 0.7965610027313232), ('vigilante', 0.7946603298187256), ('bikers', 0.7910242080688477), ('kidnap', 0.7833676934242249), ('crooked', 0.7792561650276184), ('terrorized', 0.7788373231887817), ('triad', 0.777696967124939)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.layers import Input, Conv1D, MaxPooling1D, Flatten, Dense, Embedding\n",
        "from tensorflow.keras.models import Model\n",
        "\n",
        "\n",
        "# TODO: Replace the Embedding Layer with your trained Embedding\n",
        "embedding_layer = word2vec_model.wv.get_keras_embedding(train_embeddings=False)\n",
        "\n",
        "inputs = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
        "\n",
        "x = embedding_layer(inputs)\n",
        "\n",
        "x = Conv1D(filters=128, kernel_size=4, padding='same', activation='relu')(x)\n",
        "x = MaxPooling1D(pool_size=2)(x)\n",
        "\n",
        "x = Conv1D(filters=64, kernel_size=4, padding='same', activation='relu')(x)\n",
        "x = MaxPooling1D(pool_size=2)(x)\n",
        "\n",
        "x = Conv1D(filters=32, kernel_size=4, padding='same', activation='relu')(x)\n",
        "x = MaxPooling1D(pool_size=2)(x)\n",
        "\n",
        "x = Flatten()(x)\n",
        "x = Dense(256, activation='relu')(x)\n",
        "outputs = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "model = Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "0_e85UyEPjJi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Retrain the model with 2 epochs\n",
        "\n",
        "model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=2, batch_size=BATCH_SIZE)"
      ],
      "metadata": {
        "id": "UF13TGMpPqLq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TODO: Re-evaluate the model on the test set and find accuracy\n",
        "model.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "id": "PNfzPZVgPoXk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}